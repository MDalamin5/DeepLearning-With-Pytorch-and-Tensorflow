{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2c5856c-9e5e-458d-8110-0d6de3eea71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q bitsandbytes datasets accelerate loralib\n",
    "!pip install -q git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b5cea4b-7972-4444-a781-5a7702b4381f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5f4c667-7979-4cd8-acd2-a7392b353c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"bigscience/bloom-560m\", \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df67f0fc-678e-428c-8587-d5a6184d9258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BloomForCausalLM(\n",
      "  (transformer): BloomModel(\n",
      "    (word_embeddings): Embedding(250880, 1024)\n",
      "    (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (h): ModuleList(\n",
      "      (0-23): 24 x BloomBlock(\n",
      "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): BloomAttention(\n",
      "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): BloomMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (gelu_impl): BloomGelu()\n",
      "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "652eea5a-9678-4f99-b413-57a2245ef177",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ba69b5-bace-4fd8-a034-9d7cd9794598",
   "metadata": {},
   "source": [
    "## Paramerter counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16602f75-6349-482b-8cbb-4535ea49076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2f14d-1290-4863-9247-5dc4adc2a553",
   "metadata": {},
   "source": [
    "## Obtain LoRA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef42691f-1e82-47d4-8d59-01fba9e3523c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 786432 || all params: 560001024 || trainable%: 0.14043402892063284\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model \n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd20bc-c8f4-4ef8-88f9-af29415edbff",
   "metadata": {},
   "source": [
    "## Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "903f84f4-507d-41c3-b39c-cc50ab5450ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "qa_dataset = load_dataset(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156cff8e-0a5e-4471-b9b8-00ee073d87f7",
   "metadata": {},
   "source": [
    "# **Dataset Mapping for this model**\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "329b7fe9-7f9b-447f-9aae-2400b55d8893",
   "metadata": {},
   "source": [
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### QUESTION\n",
    "{question}\n",
    "\n",
    "### ANSWER\n",
    "{answer}</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c280481-ba8a-492d-ad08-aacbe05d89ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(context, question, answer):\n",
    "  if len(answer[\"text\"]) < 1:\n",
    "    answer = \"Cannot Find Answer\"\n",
    "  else:\n",
    "    answer = answer[\"text\"][0]\n",
    "  prompt_template = f\"### CONTEXT\\n{context}\\n\\n### QUESTION\\n{question}\\n\\n### ANSWER\\n{answer}</s>\"\n",
    "  return prompt_template\n",
    "\n",
    "mapped_qa_dataset = qa_dataset.map(lambda samples: tokenizer(create_prompt(samples['context'], samples['question'], samples['answers'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a77d01-d8ac-4318-a3bf-49ef9b10e17d",
   "metadata": {},
   "source": [
    "# Trian Lora config model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f87fb7a5-193d-4e53-afbb-46274c65d790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:23, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.0, metrics={'train_runtime': 84.0996, 'train_samples_per_second': 19.025, 'train_steps_per_second': 1.189, 'total_flos': 770736984686592.0, 'train_loss': 0.0, 'epoch': 0.012277470841006752})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model, \n",
    "    train_dataset=mapped_qa_dataset[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=4, \n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=100,\n",
    "        max_steps=100,\n",
    "        learning_rate=1e-3, \n",
    "        fp16=True,\n",
    "        logging_steps=1, \n",
    "        output_dir='outputs',\n",
    "        num_train_epochs = 1.0\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2b32fd0-eb2a-4079-be10-56f389b20481",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_USER_NAME = \"alam1n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd7342b9-86e5-48fb-8f7a-2b91fbe250a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f530bcf4b5246d3a8021070b65a4577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08186baa-3c9d-4786-9596-d455c249c249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/hub.py:894: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918fa01ae6c548d29945a3ed466f6661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/alam1n/squad-bloom-588m/commit/f46dbeab1aea57e00dbe10879af75195c9af35be', commit_message='Upload model', commit_description='', oid='f46dbeab1aea57e00dbe10879af75195c9af35be', pr_url=None, repo_url=RepoUrl('https://huggingface.co/alam1n/squad-bloom-588m', endpoint='https://huggingface.co', repo_type='model', repo_id='alam1n/squad-bloom-588m'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"squad-bloom-588m\"\n",
    "\n",
    "model.push_to_hub(f\"{HUGGING_FACE_USER_NAME}/{model_name}\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f3a2396-4d1c-41a3-9b47-555625f547a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = f\"{HUGGING_FACE_USER_NAME}/{model_name}\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=False, device_map='cuda:0')\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "qa_model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb5c9697-e41f-48db-87fb-c14502b11887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define the Hugging Face user name and model name\n",
    "peft_model_id = f\"{HUGGING_FACE_USER_NAME}/{model_name}\"\n",
    "\n",
    "# Load the PEFT configuration\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# Load the base model and set it to use CUDA\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    load_in_8bit=False,  # Ensure model is not quantized for training or further modifications\n",
    "    device_map=\"cuda\"    # Use CUDA devices for the model\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the LoRA model on top of the base model\n",
    "qa_model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "\n",
    "# Ensure the LoRA model is also set to use CUDA\n",
    "qa_model = qa_model.to(\"cuda\")\n",
    "\n",
    "# Optional: Verify device placement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47b30b71-0223-42bb-8fb0-5b2fdb06281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model is loaded on device: {next(qa_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7a80f6f-3a2c-4e3c-9f9a-616f2427a65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### CONTEXT\n",
       "Cheese is the best food.\n",
       "\n",
       "### QUESTION\n",
       "What is the best food?\n",
       "\n",
       "### ANSWER\n",
       "Cheese is the best food. It is a good source of protein, vitamins, minerals, and fiber. It is also a good source of antioxidants, which are important for the health of the body. It is also a good source of fiber, which helps in the digestion of food. It is also a good source of vitamins, which are important for the health of the body. It is also a good source of antioxidants, which are important for the health of the body. It is also a good source of fiber, which helps in the digestion of food. It is also a good source of vitamins, which are important for the health of the body. It is also a good source of antioxidants, which are important for the health of the body. It is also a good source of fiber, which helps in the digestion of food. It is also a good source of vitamins, which are important for the health of the body"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def make_inference(context, question):\n",
    "    # Tokenize input and move to the same device as the model\n",
    "    batch = tokenizer(\n",
    "        f\"### CONTEXT\\n{context}\\n\\n### QUESTION\\n{question}\\n\\n### ANSWER\\n\", \n",
    "        return_tensors='pt'\n",
    "    ).to(qa_model.device)  # Move input tensors to the same device as the model\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output_tokens = qa_model.generate(**batch, max_new_tokens=200)\n",
    "\n",
    "    # Decode the output tokens and display the result\n",
    "    display(Markdown(tokenizer.decode(output_tokens[0], skip_special_tokens=True)))\n",
    "\n",
    "context = \"Cheese is the best food.\"\n",
    "question = \"What is the best food?\"\n",
    "\n",
    "make_inference(context, question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e49ec762-92a4-4c97-86a7-37d3a1e4a877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### CONTEXT\n",
       "Cheese is the best food.\n",
       "\n",
       "### QUESTION\n",
       "How far away is the Moon from the Earth?\n",
       "\n",
       "### ANSWER\n",
       "The Moon is a planet in the constellation of Sagittarius, which is the\n",
       "same as the Earth. The Moon is a planet in the constellation of Capricorn,\n",
       "which is the same as the Earth. The Moon is a planet in the constellation of\n",
       "Libra, which is the same as the Earth. The Moon is a planet in the constellation\n",
       "of Aquarius, which is the same as the Earth. The Moon is a planet in the\n",
       "constellation of Aquarius, which is the same as the Earth. The Moon is a planet\n",
       "in the constellation of Aquarius, which is the same as the Earth. The Moon is\n",
       "a planet in the constellation of Aquarius, which is the same as the Earth.\n",
       "The Moon is a planet in the constellation of Aquarius, which is the same as\n",
       "the Earth. The Moon is a planet in the constellation of Aquarius, which is\n",
       "the same as the Earth. The Moon"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context = \"Cheese is the best food.\"\n",
    "question = \"How far away is the Moon from the Earth?\"\n",
    "\n",
    "make_inference(context, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0e41d73-ac16-4e9e-804c-1eae24cdb740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### CONTEXT\n",
       "NSU is North south university\n",
       "\n",
       "### QUESTION\n",
       "How far away is the Moon from the Earth?\n",
       "\n",
       "### ANSWER\n",
       "The Moon is a planet in the constellation of Sagittarius, which is the same as the Sun. The Sun is the planet in the constellation of Capricorn, which is the same as the Moon. The Sun is the planet in the constellation of Aquarius, which is the same as the Moon. The Sun is the planet in the constellation of Capricorn, which is the same as the Moon. The Sun is the planet in the constellation of Aquarius, which is the same as the Moon. The Sun is the planet in the constellation of Aquarius, which is the same as the Moon. The Sun is the planet in the constellation of Aquarius, which is the same as the Moon. The Sun is the planet in the constellation of Aquarius, which is the same as the Moon. The Sun is the planet in the constellation of Aquarius, which is the same as the Moon. The Sun is the planet in the constellation of"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context = \"NSU is North south university\"\n",
    "question = \"How far away is the Moon from the Earth?\"\n",
    "\n",
    "make_inference(context, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf95be-e306-4461-aa45-3dce24e5b583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
