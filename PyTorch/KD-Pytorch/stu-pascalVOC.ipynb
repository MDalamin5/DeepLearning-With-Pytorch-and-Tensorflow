{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b3b76c1-4ab8-43ce-a6e2-5ef40da5073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Required Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaeceed4-25a4-4064-9ae4-bb2bce2c0e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define a lightweight CNN model\n",
    "class LightNet(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super(LightNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 128)  # Adjust according to your input image size\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = LightNet(num_classes=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e00cd763-e990-4745-863f-c6caa3da757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define a custom collate function\n",
    "def custom_collate_fn(batch):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for item in batch:\n",
    "        img, target = item  # img is the image, target is the dictionary\n",
    "        images.append(img)\n",
    "\n",
    "        # Create an empty label array for the image\n",
    "        label_array = torch.zeros(20, dtype=torch.float32)  # Assuming 20 classes\n",
    "        if 'object' in target['annotation']:\n",
    "            for obj in target['annotation']['object']:\n",
    "                class_name = obj['name']\n",
    "                if class_name in class_mapping:\n",
    "                    class_id = class_mapping[class_name]\n",
    "                    label_array[class_id] = 1  # Mark this class as present\n",
    "\n",
    "        labels.append(label_array)\n",
    "\n",
    "    return torch.stack(images), torch.stack(labels)  # Stack images and labels to form batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ffb3595-cc45-4114-93bc-3209f3276b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a2590a4-a5bd-4207-be85-c1dbce5fa075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: voc-data\\VOCtrainval_11-May-2012.tar\n",
      "Extracting voc-data\\VOCtrainval_11-May-2012.tar to voc-data\n",
      "Using downloaded and verified file: voc-data\\VOCtrainval_11-May-2012.tar\n",
      "Extracting voc-data\\VOCtrainval_11-May-2012.tar to voc-data\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Load the Pascal VOC dataset\n",
    "root_path = 'voc-data'  # Define the root path for downloading the dataset\n",
    "\n",
    "# Load the training and validation sets\n",
    "train_set = torchvision.datasets.VOCDetection(root=root_path, year='2012', image_set='train', transform=transform, download=True)\n",
    "val_set = torchvision.datasets.VOCDetection(root=root_path, year='2012', image_set='val', transform=transform, download=True)\n",
    "\n",
    "# Create DataLoaders for the training and validation sets\n",
    "# train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(val_set, batch_size=16, shuffle=False)\n",
    "\n",
    "# Step 2: Update DataLoader to use the custom collate function\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=16, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d14e3f5f-cc83-4d6e-99d9-d35fbd234d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7e39536-1e6b-41d0-89b3-a048738acd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Define the class mapping\n",
    "class_mapping = {\n",
    "    'aeroplane': 0,\n",
    "    'bicycle': 1,\n",
    "    'bird': 2,\n",
    "    'boat': 3,\n",
    "    'bottle': 4,\n",
    "    'bus': 5,\n",
    "    'car': 6,\n",
    "    'cat': 7,\n",
    "    'chair': 8,\n",
    "    'cow': 9,\n",
    "    'diningtable': 10,\n",
    "    'dog': 11,\n",
    "    'horse': 12,\n",
    "    'motorbike': 13,\n",
    "    'person': 14,\n",
    "    'pottedplant': 15,\n",
    "    'sheep': 16,\n",
    "    'sofa': 17,\n",
    "    'train': 18,\n",
    "    'tvmonitor': 19\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c64585d-9552-4c9f-b873-5202c0962c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LightNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=200704, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7: Set device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e9c05c-fa89-42a9-84db-d839c88b0f08",
   "metadata": {},
   "source": [
    "## Model Train and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec381139-b2a7-4a90-9af2-ebed60170164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation function\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for images, targets in val_loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, targets)  # Calculate loss\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)  # Accumulate loss\n",
    "\n",
    "            # Calculate predictions and compare with targets\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(targets, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    accuracy = 100 * correct / total  # Calculate accuracy\n",
    "\n",
    "    return val_loss, accuracy\n",
    "\n",
    "# Step 8: Run validation after training epochs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "602d2ebf-f780-4410-bbad-d9fdfe4c6239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 3.4214\n",
      "Epoch [1/10], Validation Loss: 3.7693, Validation Accuracy: 20.80%\n",
      "Epoch [2/10], Loss: 2.6927\n",
      "Epoch [2/10], Validation Loss: 4.0838, Validation Accuracy: 20.57%\n",
      "Epoch [3/10], Loss: 2.0619\n",
      "Epoch [3/10], Validation Loss: 4.5623, Validation Accuracy: 20.59%\n",
      "Epoch [4/10], Loss: 1.7542\n",
      "Epoch [4/10], Validation Loss: 4.5669, Validation Accuracy: 19.54%\n",
      "Epoch [5/10], Loss: 1.5765\n",
      "Epoch [5/10], Validation Loss: 5.0700, Validation Accuracy: 19.23%\n",
      "Epoch [6/10], Loss: 1.4667\n",
      "Epoch [6/10], Validation Loss: 4.8686, Validation Accuracy: 18.62%\n",
      "Epoch [7/10], Loss: 1.3611\n",
      "Epoch [7/10], Validation Loss: 4.7417, Validation Accuracy: 19.94%\n",
      "Epoch [8/10], Loss: 1.3345\n",
      "Epoch [8/10], Validation Loss: 5.1274, Validation Accuracy: 19.32%\n",
      "Epoch [9/10], Loss: 1.2849\n",
      "Epoch [9/10], Validation Loss: 5.0594, Validation Accuracy: 18.91%\n",
      "Epoch [10/10], Loss: 1.2660\n",
      "Epoch [10/10], Validation Loss: 4.8336, Validation Accuracy: 19.58%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)  # Move targets to device\n",
    "\n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "\n",
    "        outputs = model(images)  # Forward pass\n",
    "\n",
    "        # Compute loss (assuming outputs are [batch_size, num_classes])\n",
    "        loss = criterion(outputs, targets)  # Calculate loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}')\n",
    "    # Training code here\n",
    "\n",
    "    \n",
    "    # Run validation after each epoch\n",
    "    val_loss, accuracy = validate(model, val_loader, criterion, device)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea944e94-bb43-4e78-b14c-622538698435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
